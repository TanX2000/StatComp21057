---
title: 'Introduction to SIRS'
author: "Tan Xin"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to SIRS}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__StatComp21057__ is a R package devoted to sparse recovery. Two functions called _sirs_ () and _cv.sirs_ () are considered. The first function _sirs_() is an implementation of the sequentially and iteratively reweighted squares (SIRS) algorithm, which gives the sparsest regularized least square solution using a penalty family(SICA) between L0 and L1. And _cv.sirs_() is a cross-validation procedure for SIRS algorithm to select the best parameter a of the penalty function $\rho_a$.


## sirs

For sparse recovery, we consider a $\rho$-regularization problem:
$$
\min \sum_{j=1}^{p} \rho_a(|\beta_j|) \quad \mbox{subject to} \quad y=X\beta
$$
where $\rho_a(t) = \frac{(a+1)t}{a+t}$.

We propose the sequentially and iteratively reweighted squares (SIRS) algorithm
for solving the above probelm. The corresponding SIRS algorithm is as follows:

Firstly, pick a level of sparsity S, the number of iterations L, the number	of sequential steps $M \leq S$ and a small constant $\epsilon \in (0,1)$.

* (1) Set k=0

* (2) Initialize ${\beta}^{(0)}=\mathbf{1}$ and set $\mathcal{l}=1$.
 
* (3) Set ${\beta}^{(\mathcal{l})} \gets \mathbf{v}({\beta}^{(\mathcal{l}-1)})$ with $\mathbf{D}=\mathbf{D}({\beta}^{(\mathcal{l}-1)})$  and $\mathcal{l} \gets \mathcal{l}+1$., where

		$$
		\mathbf{v}({\beta})=\mathbf{D}\mathbf{X}^{T}(\mathbf{X}\mathbf{D}\mathbf{X}^T)^{+}\mathbf{y},\quad \mathbf{D}=\mathbf{D}({\beta})=diag\{d_1,...,d_p\}
		$$
		
		$$
		d_j=\beta_j^2/\rho_a(|\beta_j|)=(a+1)^{-1}|\beta_j|(a+|\beta_j|),j=1,...,p
		$$

* (4) Repeat step 3 until convergence or $\mathcal{l}=L+1$. Denote by $\tilde{{\beta}}$ the resulting p vector.

* (5) If $||\tilde{{\beta}}||_0 \leq S$, stop and return $\tilde{{\beta}}$. Otherwise, set $k \gets k+1$ and repeat steps 2â€“4 with ${\beta}^{(0)}=I(|\tilde{{\beta}} \geq \gamma_k|)+\epsilon I(|\tilde{{\beta}}< \gamma_k|)$ and $\gamma_k$ the kth largest component of
		$|\tilde{{\beta}}|$, until stop or k=M. Return $\tilde{{\beta}}$.

For step 1-4, we implement it in a subfunction _sirscore_(). Then we give a complete implementation of this algorithm by _sirs_().
The source R code for _sirs_ is as follows:

```{r,eval=TRUE}
sirs<-function(X, y, a = 0.1) {
  n <- length(X[ ,1])
  p <- length(X[1, ])
  b0 <- rep(1, p)
  S <- min(floor(n/2), p)
  eps <- 1/p
  thresh <- 1e-6
  L <- 50
  M = min(L, S)
  tol = 1e-6
  k = 1
  diff = 1
  b.old = numeric(p)
  
  sirscore <- function(X, y, a = 0.1, b0 = rep(1, length(X[1, ]))){
    delta <- 1e-6
    dj <- function(t) abs(t) * (a + abs(t))/(a+1)
    D <- diag(sapply(b0, dj), p, p)
    b <- b0
    diff <- 1
    l <- 1
    dj <- function(t) abs(t) * (a + abs(t))/(a+1)
    D <- diag(sapply(b0, dj), p, p)
    while (diff > tol & l <= L){
      l = l + 1
      bold = b
      b = D%*%t(X)%*%(solve(diag(delta, n, n) + X%*%D%*%t(X)))%*%y
      diff = sqrt(sum((b - bold)^2))
      D = diag(sapply(b, dj), p, p)
    }
    b = b * (abs(b) > thresh)
    
    if (sum(b != 0) <= S){
      b_non0 = which(b != 0)
      X_non0 = X[, b_non0]
      b[b_non0] = solve(t(X_non0)%*%X_non0)%*%t(X_non0)%*%y
      b = b * (abs(b) > thresh)
    }
    return (b)
  }
  
  while (diff > tol & k <= M){
    b = sirscore(X, y, a, b0)
    l0 = sum(b != 0)
    if (l0 <= S){
      break
    }
    else {
      b_non0 = which(b != 0)
      index = order(abs(b[b_non0]), decreasing = TRUE)
      
      b_ini = rep(eps, p)
      b_ini[b_non0[index[1:k]]] = 1
      
      diff = sqrt(sum((b - b.old)^2))
      b.old = b
      k = k + 1
    }
  }
  return (b)
}
```

Here we give an example of the usage of _sirs_().

## cv.sirs

To choose the best parameter a of penalty function $\rho_a$, we consider a k-folds cross-validation function called _cv.sirs_(). The The source R code for _cv.sirs_() is as follows:

```{r, eval = TRUE}
cv.sirs <- function(X, y, k = 10, a = seq(0,1,0.1)){
  n <- length(X[, 1])
  cvgroup <- function(k, datasize){
    cvlist <- list()
    n <- rep(1:k, ceiling(datasize/k))[1:datasize] 
    temp <- sample(n, datasize)
    x <- 1:k
    dataseq <- 1:datasize
    cvlist <- sapply(x, function(x) {dataseq[temp==x]})
    return(cvlist)
  }
  
  loss_rho <- function(a, beta){
    rho_a <- function(t) abs(t) * (a + abs(t))/(a+1)
    loss <- rho_a(beta[beta!=0])
    return (sum(loss))
  }
  
  loss <- numeric(length(a))
  cvlist <- cvgroup(k, n)
  
  loss <-sapply((1:length(a)), FUN = function(i){
    tmp<-sapply(1:k, function(j){
      Xj = X[-cvlist[[j]],]
      yj = y[-cvlist[[j]]]
      beta = sirs(Xj, yj, a[i])
      return (beta)
    })
    return (sum(apply(tmp, MARGIN = 2, FUN = function(beta){loss_rho(a[i], beta)})))
  })
  a.best <- a[which.min(loss)]
  return (a.best)
}
```

Here we give an example of the usage of _sirs_() and _cv.sirs_().

```{r,echo = TRUE}
library(MASS)
set.seed(12345)
n <- 35
p <- 500
beta_0 <- numeric(p)
beta_0[1:7] <- c(1, -0.5, 0.7,-1.2, -0.9, 0.3, 0.55)
r <- 0.1
gamma_r <- matrix(r, p, p) + diag(1 - r, p, p)
X <- mvrnorm(n, numeric(p), gamma_r)
y <- X%*%beta_0
k <- 2
a <- c(0.2, 0.3)
a.best <- (cv.sirs(X, y, k, a))
print(a.best)
beta_hat <- sirs(X, y, a.best)
print(sum(beta_hat != 0))
print(which(beta_hat != 0))
```

From the result generated, we can see that _sirs_ indeed selects the underlying true variables and recovers the coefficients correctly.